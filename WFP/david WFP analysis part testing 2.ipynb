{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5    \n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@yanlinc/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://methods.sagepub.com/base/download/DatasetHowToGuide/latent-dirichlet-allocation-in-news-2016-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('C:/Users/jeanl/Desktop/trial.csv',index_col=False)\n",
    "from textblob import TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>words</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>words_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WFP</td>\n",
       "      <td>FOOD SECURITY \\n AND NUTRITION \\nIN THE WORLD\\...</td>\n",
       "      <td>[FOOD, SECURITY, AND, NUTRITION, IN, THE, WORL...</td>\n",
       "      <td>FOOD SECURITY AND NUTRITION IN THE WORLD THE S...</td>\n",
       "      <td>[FOOD, SECURITY, AND, NUTRITION, IN, THE, WORL...</td>\n",
       "      <td>[food, security, and, nutrition, in, the, worl...</td>\n",
       "      <td>[food, security, and, nutrition, in, the, worl...</td>\n",
       "      <td>[food, security, nutrition, world, state, safe...</td>\n",
       "      <td>[(food, NN), (security, NN), (nutrition, NN), ...</td>\n",
       "      <td>[(food, n), (security, n), (nutrition, n), (wo...</td>\n",
       "      <td>[food, security, nutrition, world, state, safe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster                                              words  \\\n",
       "0     WFP  FOOD SECURITY \\n AND NUTRITION \\nIN THE WORLD\\...   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [FOOD, SECURITY, AND, NUTRITION, IN, THE, WORL...   \n",
       "\n",
       "                                           words_str  \\\n",
       "0  FOOD SECURITY AND NUTRITION IN THE WORLD THE S...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [FOOD, SECURITY, AND, NUTRITION, IN, THE, WORL...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [food, security, and, nutrition, in, the, worl...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [food, security, and, nutrition, in, the, worl...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [food, security, nutrition, world, state, safe...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(food, NN), (security, NN), (nutrition, NN), ...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [(food, n), (security, n), (nutrition, n), (wo...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [food, security, nutrition, world, state, safe...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"C:/Users/mwamb/OneDrive/Desktop/tenthdimensionanalytics/WFP/processed_data/wfpdata_clean\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'] = df['words'].apply(lambda x: TextBlob(x).polarity)\n",
    "df['subjective'] = df['words'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english') + ['though','www','https']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of ngrams\n",
    "ngrams = c_vec.fit_transform(df['words'])\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)).rename(columns={0: 'frequency', 1:'bigram/trigram'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>bigram/trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>949</td>\n",
       "      <td>food insecurity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>615</td>\n",
       "      <td>food security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>433</td>\n",
       "      <td>acute food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>392</td>\n",
       "      <td>acute food insecurity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>368</td>\n",
       "      <td>food crises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227313</td>\n",
       "      <td>1</td>\n",
       "      <td>00 11 51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227314</td>\n",
       "      <td>1</td>\n",
       "      <td>00 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227315</td>\n",
       "      <td>1</td>\n",
       "      <td>00 10 3n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227316</td>\n",
       "      <td>1</td>\n",
       "      <td>00 10 1swedenn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227317</td>\n",
       "      <td>1</td>\n",
       "      <td>00 10 1latvian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227318 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frequency         bigram/trigram\n",
       "0             949        food insecurity\n",
       "1             615          food security\n",
       "2             433             acute food\n",
       "3             392  acute food insecurity\n",
       "4             368            food crises\n",
       "...           ...                    ...\n",
       "227313          1               00 11 51\n",
       "227314          1                  00 11\n",
       "227315          1               00 10 3n\n",
       "227316          1         00 10 1swedenn\n",
       "227317          1         00 10 1latvian\n",
       "\n",
       "[227318 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram['polarity'] = df_ngram['bigram/trigram'].apply(lambda x: TextBlob(x).polarity)\n",
    "df_ngram['subjective'] = df_ngram['bigram/trigram'].apply(lambda x: TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.0\n",
       "1         0.0\n",
       "2         0.9\n",
       "3         0.9\n",
       "4         0.0\n",
       "         ... \n",
       "227313    0.0\n",
       "227314    0.0\n",
       "227315    0.0\n",
       "227316    0.0\n",
       "227317    0.0\n",
       "Name: subjective, Length: 227318, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ngram['subjective']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF models\n",
    "Non-Negative Matrix Factorization (NMF) is a matrix decomposition method, which decomposes a matrix into the product of W and H of non-negative elements. The default method optimizes the distance between the original matrix and WH, i.e., the Frobenium norm. Below is an example where we use NMF to produce 3 topics and we showed 3 bigrams/trigrams in each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4c96c24c8daa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstoplist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnmf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "nmf = NMF(n_components=10)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(df['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic %d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        \n",
    "        \n",
    "        print(message)\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8ebf7a4e48da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnmf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nmf' is not defined"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA models\n",
    "\n",
    "   \n",
    "Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(2, 3), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourse...\n",
       "                ('latentdirichletallocation',\n",
       "                 LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                                           evaluate_every=-1,\n",
       "                                           learning_decay=0.7,\n",
       "                                           learning_method='batch',\n",
       "                                           learning_offset=10.0,\n",
       "                                           max_doc_update_iter=100, max_iter=10,\n",
       "                                           mean_change_tol=0.001,\n",
       "                                           n_components=3, n_jobs=None,\n",
       "                                           perp_tol=0.1, random_state=None,\n",
       "                                           topic_word_prior=None,\n",
       "                                           total_samples=1000000.0,\n",
       "                                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))\n",
    "lda = LatentDirichletAllocation(n_components=3)\n",
    "pipe = make_pipeline(tfidf_vectorizer, lda)\n",
    "pipe.fit(df['words'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: food security, food insecurity, acute food, acute food insecurity, food crises\n",
      "Topic #1: food security, food insecurity, acute food, acute food insecurity, food crises\n",
      "Topic #2: food insecurity, food security, acute food, acute food insecurity, food crises\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tfidf_vectorizer.get_feature_names(), n_top_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"player players football game match david tenis 'df'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player True 7.1004224 False\n",
      "players True 7.18582 False\n",
      "football True 7.4812903 False\n",
      "game True 7.441479 False\n",
      "match True 6.2072406 False\n",
      "david True 6.5919833 False\n",
      "tenis True 7.365459 False\n",
      "' True 6.137929 False\n",
      "df True 6.2081633 False\n",
      "' True 6.137929 False\n"
     ]
    }
   ],
   "source": [
    "for token in tokens: \n",
    "    # Printing the following attributes of each token. \n",
    "    # text: the word string, has_vector: if it contains \n",
    "    # a vector representation in the model,  \n",
    "    # vector_norm: the algebraic norm of the vector, \n",
    "    # is_oov: if the word is out of vocabulary. \n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: <built-in method similarity of spacy.tokens.doc.Doc object at 0x0000021A55567F48>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Similarity:\", tokens.similarity(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\" i love football\")\n",
    "doc2 = nlp(\" football game is nie\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7299463936488044\n"
     ]
    }
   ],
   "source": [
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40540856\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"we love our parents\")\n",
    "\n",
    "token1=doc[3]\n",
    "token2= doc[1]\n",
    "\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"let's start the game \")\n",
    "token = nlp(\"play\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6587394569716348\n"
     ]
    }
   ],
   "source": [
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6792010226689111\n"
     ]
    }
   ],
   "source": [
    "text1 = nlp(\"let's meet at java and discuss more\")[2:5]\n",
    "text2 = nlp(\" we ca have our meeting at a restaurant\")\n",
    "\n",
    "print(text1.similarity(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3336e-01  2.5781e-01 -2.0162e-01  5.2606e-01 -2.8509e-02 -1.1073e-01\n",
      "  1.2757e-02 -2.0570e-01 -2.1759e-01  2.9903e+00 -3.4121e-01 -3.4941e-01\n",
      "  1.5605e-01  3.1501e-03  4.7926e-02  6.0154e-02 -2.5585e-01  5.3310e-01\n",
      " -5.2395e-01 -2.0549e-01  3.9880e-01  9.4371e-02 -8.1085e-03 -1.4268e-01\n",
      "  6.9565e-02  1.9760e-01  3.3227e-01  2.6695e-01  1.2351e-01 -2.4241e-01\n",
      "  3.0009e-02  1.3260e-01  1.9259e-01  2.2053e-01  2.2206e-01  4.1598e-01\n",
      "  4.7964e-01  1.1994e-01 -4.0417e-01 -2.7630e-01 -3.4020e-01 -3.0179e-01\n",
      " -6.7554e-02  1.0889e-01 -3.3474e-01  3.0955e-01  3.2112e-02  2.5001e-01\n",
      "  1.2119e-01 -2.1610e-01  2.8064e-01  1.6946e-01 -2.1553e-01 -5.8009e-02\n",
      " -5.2865e-02  5.2013e-02 -4.4257e-01 -2.5601e-01 -8.0894e-02  1.9187e-01\n",
      "  3.5572e-01  2.6406e-01 -4.4727e-02  4.0338e-01  2.8569e-01 -2.7695e-01\n",
      "  1.7033e-01  1.4237e-01  1.0442e-01 -7.8184e-02  2.5931e-01 -1.5034e-01\n",
      "  2.0141e-01 -2.1194e-01  2.6104e-02  2.4404e-01 -4.0048e-02 -3.8523e-01\n",
      "  1.8870e-01  1.3488e-01  2.8087e-01 -2.4757e-01 -3.8005e-01  1.4582e-01\n",
      " -3.5891e-01 -1.1986e-01 -9.8088e-01 -1.7491e-01  2.7451e-01 -1.5003e-01\n",
      " -1.8748e-01 -5.2634e-01 -3.3806e-01 -3.3667e-02 -1.8723e-01  3.8529e-01\n",
      " -2.0939e-01 -2.8131e-01  9.2191e-02 -3.2378e-01  2.0437e-01  1.6642e-01\n",
      "  4.7932e-01 -5.2379e-02  1.0176e-01 -4.8485e-01  6.7107e-02 -2.0908e-01\n",
      " -2.6838e-01  1.3176e-01  8.9185e-02 -9.5929e-02  8.6467e-02  5.1444e-01\n",
      "  2.7215e-01 -3.0075e-01 -2.9866e-01 -3.9815e-01  1.4000e-01  1.9878e-01\n",
      "  3.8434e-01 -4.1152e-01 -1.4932e-01  3.2557e-01  5.5985e-01  5.2648e-02\n",
      "  1.8611e-01  3.6447e-04  2.6251e-01 -6.9239e-02  4.6839e-03  2.4047e-03\n",
      " -2.2873e-01 -2.7271e-02 -2.7025e-01  5.1928e-01  1.0296e-01 -3.0266e-02\n",
      "  4.5503e-01  3.0211e-01 -1.4351e-01 -3.7883e-01 -5.3203e-01 -1.7350e-01\n",
      "  4.4668e-01  5.9103e-02 -1.2249e-01 -1.0916e-01 -2.1118e-01  3.0307e-02\n",
      "  9.7751e-02  4.3791e-01 -2.4311e-01 -3.2299e-01 -6.9884e-02  4.5493e-02\n",
      "  5.9009e-01 -3.5096e-01 -2.1166e-01  2.8629e-01 -4.0601e-01 -5.6530e-02\n",
      " -4.9276e-01 -6.2874e-01  4.2936e-01 -5.4930e-02 -7.7768e-03 -7.3195e-02\n",
      " -6.8264e-02 -7.0292e-02 -2.5423e-01  5.4630e-02  3.9436e-01  1.8735e-02\n",
      "  6.6797e-02  1.3017e-01  9.0828e-02  4.8007e-01  3.3149e-01  1.4041e-01\n",
      " -2.5465e-01  5.7773e-02  3.2110e-01  7.4905e-02  4.2335e-01 -3.9085e-02\n",
      " -2.6920e-01 -2.0436e-01 -2.6450e-01  4.3559e-01  5.9239e-02 -1.3725e-01\n",
      " -9.4348e-02 -1.9529e-01 -2.1799e-01  3.2888e-01 -2.5791e-03  1.8041e-01\n",
      "  1.3963e-01  3.0054e-01  4.1881e-01  2.4218e-01 -3.7679e-02 -2.5239e-01\n",
      "  1.0401e-01  3.2577e-02 -3.2033e-01  2.1195e-01 -1.1134e-01  4.0074e-01\n",
      "  2.5524e-02 -2.2245e-01  8.0663e-02 -4.9761e-01 -1.5854e-01  6.4592e-03\n",
      " -1.8710e-01  5.7463e-01 -2.7990e-01  5.7908e-02  1.2323e-04 -3.4725e-01\n",
      " -3.9925e-01  9.5676e-02 -9.3906e-02 -5.4630e-02 -1.4233e-01 -4.0527e-01\n",
      " -1.9574e-02 -3.4353e-01 -2.1680e-01  2.0505e-01  6.8026e-03  1.2003e-01\n",
      " -1.2370e-01  1.9338e-01 -4.1974e-01  2.1074e-01  1.5990e-01  1.7787e-01\n",
      "  2.2786e-01 -3.7224e-01  7.8567e-02 -6.8207e-01 -5.2561e-02  2.3565e-01\n",
      "  2.2454e-01  3.8459e-01 -5.1981e-02 -3.4568e-01 -1.1119e-01 -2.5117e-01\n",
      " -1.1288e-01 -3.0129e-01  3.9091e-01 -5.9062e-01 -1.4043e-01 -2.4592e-01\n",
      " -2.0647e-01 -1.7632e-02 -5.5266e-02  1.6582e-01  1.5932e-01  3.4474e-01\n",
      "  4.4176e-02 -3.7045e-01 -3.5199e-01  2.0974e-01  6.9742e-01  4.6268e-01\n",
      " -8.5699e-02  2.5818e-01  2.5318e-01 -1.9820e-01 -1.9635e-02  1.6738e-01\n",
      "  3.6973e-02  5.1333e-01 -2.5764e-01  2.5194e-01  1.9789e-01 -3.3666e-02\n",
      " -1.1882e-01 -4.5939e-02 -2.5952e-01  3.4509e-01  1.1083e-01  2.8321e-01\n",
      " -2.7592e-01  4.4470e-01  1.2571e-02 -3.0703e-02 -3.5869e-01 -1.0422e-01\n",
      " -1.8774e-01 -3.4711e-01  3.2193e-02 -2.3186e-01  5.1135e-02  4.2740e-01]\n"
     ]
    }
   ],
   "source": [
    "print(text1[4].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we True 5.260865 False\n",
      "love True 6.04035 False\n",
      "our True 5.592738 False\n",
      "lives True 6.036322 False\n",
      "Similarity: 0.5461829\n"
     ]
    }
   ],
   "source": [
    " \n",
    "words = \"we love our lives\" \n",
    "  \n",
    "tokens = nlp(words) \n",
    "  \n",
    "for token in tokens: \n",
    "    # Printing the following attributes of each token. \n",
    "    # text: the word string, has_vector: if it contains \n",
    "    # a vector representation in the model,  \n",
    "    # vector_norm: the algebraic norm of the vector, \n",
    "    # is_oov: if the word is out of vocabulary. \n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) \n",
    "  \n",
    "token1, token2 = tokens[0], tokens[1] \n",
    "  \n",
    "print(\"Similarity:\", token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling with bigram \n",
    "\n",
    "### NMF models Non-Negative Matrix Factorization (NMF) \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,3))\n",
    "nmf = NMF(n_components=5)\n",
    "pipe = make_pipeline(tfidf_vectorizer, nmf)\n",
    "pipe.fit(nouns)\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic %d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        \n",
    "        \n",
    "        print(message)\n",
    "        \n",
    "    print()\n",
    "\n",
    "\n",
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=7)\n",
    "\n",
    "\n",
    "## Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,3))\n",
    "lda = LatentDirichletAllocation(n_components=3)\n",
    "pipe = make_pipeline(tfidf_vectorizer, lda)\n",
    "pipe.fit(nouns)\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        words = \"Topic %d: \" % topic_idx\n",
    "        words += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(words)\n",
    "    print()\n",
    "\n",
    "print_top_words(lda, tfidf_vectorizer.get_feature_names(), n_top_words=6)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
